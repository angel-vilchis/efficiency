{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e999338-cb52-4e74-a778-5e230a60e89e",
   "metadata": {},
   "source": [
    "# [Do Deep Nets Really Need to be Deep?](https://arxiv.org/pdf/1312.6184)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75ca51-04dd-4259-9582-5edd22622bf1",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5dc40-3866-4dc6-a69f-ea7c15504451",
   "metadata": {},
   "source": [
    "* Since it has been shown we can mimic the function learned by complex model with a small net, the function learned by complex model wasn't truly too complex to be learned by the small net\n",
    "    * Showing that shallow models are capable of learning the same function as deep nets debunks the myth that function learned by deep net has to be deep. <br><br>    \n",
    "* It is better to train a student model on logits since different logits can map to same distribution when using softmax (technically losing information the complex model learned)\n",
    "    * Also, softmax can lead to few large values relative to others, which would cause cross entropy to focus on them, ignoring others <br><br>\n",
    "* Shallow and wide models are slower for learning since there are many highly correlated features, so gradient descent converges slowly\n",
    "    * One remedy is linear bottleneck (weight matrix factorization), which only increases speed and doesn't increase representational power <br><br>\n",
    "* Why training on teacher model's prediction could be better than training directly on original dataset can be due to:\n",
    "    * Teacher model can eliminate label errors by predicting those correctly (from generalization)\n",
    "    * Teacher model soft targets provide more information than hard targets such as confusable classes <br><br>\n",
    "* Model compression works best when unlabeled data set is much larger than train set (to reduce gap between teacher and student) and when unlabeled examples aren't training points (teacher model is more likely to have overfit these)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fdb6b36-4937-423a-9fc6-9034f5c9810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.functional.softmax(torch.tensor([-10.0, 0.0, 10.0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9f4f3e5-34f1-42cb-a8c5-b54aa1b91bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([10.0, 20.0, 30.0]), dim=-1) # Different logits same softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf74e32-ee11-40c7-bd56-98792523d052",
   "metadata": {},
   "source": [
    "Say target is $[3.0385e^{-7}, 6.6928e^{-3}, 9.9331e^{-1}]$ and prediction is $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$\n",
    "\n",
    "$CE_{Loss} \\approx -[3.0e^{-7} \\ log(\\frac{1}{3}) + 6.7e^{-3} \\ log(\\frac{1}{3}) + 9.9e^{-1} \\ log(\\frac{1}{3})] = $\n",
    "\n",
    "We can see most of the loss would come from largest target so model would focus on getting that right and ignoring others targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
