{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e999338-cb52-4e74-a778-5e230a60e89e",
   "metadata": {},
   "source": [
    "# [Do Deep Nets Really Need to be Deep?](https://arxiv.org/pdf/1312.6184)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75ca51-04dd-4259-9582-5edd22622bf1",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b5dc40-3866-4dc6-a69f-ea7c15504451",
   "metadata": {},
   "source": [
    "* Since it has been shown we can mimic function learned by complex model with a small net, the function learned by complex model wasn't truly too complex to be learned by small net\n",
    "* It is better to train a student on logits since different logits can map to same distribution when using softmax (technically losing information the complex model learned) and softmax can lead to relatively larger values which would cause cross entropy to focus on them, ignoring others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fdb6b36-4937-423a-9fc6-9034f5c9810a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.nn.functional.softmax(torch.tensor([-10.0, 0.0, 10.0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9f4f3e5-34f1-42cb-a8c5-b54aa1b91bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0611e-09, 4.5398e-05, 9.9995e-01])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([10.0, 20.0, 30.0]), dim=-1) # Different logits same softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf74e32-ee11-40c7-bd56-98792523d052",
   "metadata": {},
   "source": [
    "Say target is $[3.0385e^{-7}, 6.6928e^{-3}, 9.9331e^{-1}]$ and prediction is $[\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]$\n",
    "\n",
    "$CE_{Loss} \\approx -[3.0e^{-7} \\ log(\\frac{1}{3}) + 6.7e^{-3} \\ log(\\frac{1}{3}) + 9.9e^{-1} \\ log(\\frac{1}{3})] = $\n",
    "\n",
    "We can see most of the loss would come from largest target so model would focus on getting that right and ignoring others targets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
